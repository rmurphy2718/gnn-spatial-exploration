{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with a Graph Neural Network\n",
    "\n",
    "Now, we pose this as a vertex regression task.  That is, we have a graph $G = (V, E)$ with associated vertices $V = \\{1, 2, \\ldots, 456 \\}$ representing locations and edges $E$ representing nearest neighbor relationships.  Each vertex $v \\in V$ is also associated with an input feature $x_v \\in \\mathbb{R}$ representing the (scaled) altitude as well as a target value $y_v \\in \\mathbb{R}$ representing the precipitation.  We will use a [Graph Convolutional Network](https://arxiv.org/abs/1609.02907) to predict the $y_v$ values.\n",
    "\n",
    "## Loading the data\n",
    "\n",
    "We will load the data pre-processed in R into a Pytorch Geometric InMemoryDataset using the [convert_pytorch_geometric](https://github.com/rmurphy2718/gnn-spatial-exploration/blob/main/convert_pytorch_geometric.py) script.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8468146750>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms as transforms\n",
    "from convert_pytorch_geometric import CaRain, create_pyg\n",
    "\n",
    "torch.manual_seed(42)  # Set a seed for consistency in blogging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 2772], x=[456, 13], y=[456])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = CaRain(\".\", pre_transform=create_pyg)\n",
    "g = dataset[0]\n",
    "\n",
    "# Note: nearest neighbors graph is not regular; that is, not all vertices have the same degree.\n",
    "# So, we can add a one-hot encoding of the vertex degree as an additional feature as is sometimes done with featureless graphs, \n",
    "# in case it carries extra useful information.\n",
    "degrees = torch_geometric.utils.degree(g.edge_index[0])\n",
    "max_deg = torch.max(degrees).long().item()\n",
    "add_degree_fn = transforms.OneHotDegree(max_deg)\n",
    "add_degree_fn(g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we partition the vertex set into train and test sets.  We will leave it at that, although using multiple random sets and model initializations would provide a better assessment of generalization performance.  I will update this blog if I make this enhancement.\n",
    "\n",
    "Specifically, we create boolean indicators -- masks -- that indicate whether a vertex belongs to a given set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_masks(data_size, train_size):\n",
    "    # Creat a boolean with exactly `train_size` are True, rest False\n",
    "    unshuffled_train = int(train_size) > torch.arange(data_size)\n",
    "    \n",
    "    # shuffle to get train mask\n",
    "    train_mask = unshuffled_train[torch.randperm(data_size)]\n",
    "    \n",
    "    # negate to get test mask\n",
    "    test_mask = ~train_mask\n",
    "    \n",
    "    return train_mask, test_mask\n",
    "\n",
    "g.train_mask, g.test_mask = make_train_test_masks(g.num_nodes, 0.2 * g.num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a GNN model\n",
    "\n",
    "Next, we initialize an off-the-shelf Graph Convolutional Network given in the [PyTorch Geometric tutorial](https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html).  I wonder whether this is enough to do better than the global regression?  In my experience, while refined Message Passing GNNs have been developed since GCN, it performs reasonably well and is a simple choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = GCNConv(in_dim, 16)\n",
    "        self.conv2 = GCNConv(16, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)  # In my experience, weaker dropout is good\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using the Adam optimizer for 5000 epochs.  Even on my local machine's CPUs, it only takes a few moments to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515239.75\n",
      "162703.046875\n",
      "150588.046875\n",
      "162187.421875\n",
      "152907.640625\n",
      "168840.875\n",
      "156450.796875\n",
      "153671.765625\n",
      "145880.078125\n",
      "145020.8125\n"
     ]
    }
   ],
   "source": [
    "model = Model(g.num_node_features)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01, weight_decay=5e-4)\n",
    "crit = nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "for epo in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(g)\n",
    "        \n",
    "    loss = crit(pred[g.train_mask], g.y[g.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epo % 500 == 0:\n",
    "        print(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the MSE values are large due to the scale of the target.  Finally, we can evaluate the model on the vertices in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184472.328125\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_pred = model(g)\n",
    "test_loss = crit(pred[g.test_mask], g.y[g.test_mask]).item()\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with global linear regression\n",
    "\n",
    "The [Spatial Data Anlysis textbook](https://rspatial.org/raster/analysis/analysis.pdf) uses (global) linear regression as the naive baseline.  Let us see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186334.109375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numpy\n",
    "X = g.x[:, 0].unsqueeze(1).numpy()  # Remove degree information\n",
    "y = g.y.numpy()\n",
    "train_mask = g.train_mask.numpy()\n",
    "\n",
    "# Train on training split (using altitude only)\n",
    "lm = LinearRegression()\n",
    "reg = lm.fit(X[train_mask], y[train_mask])\n",
    "\n",
    "# Evaluate on test split\n",
    "yhat = reg.predict(X[~train_mask])\n",
    "crit(torch.from_numpy(yhat), g.y[g.test_mask]).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and discussion\n",
    "\n",
    "Pulling from the results above, the Mean Squared Errors for the two models are shown below.\n",
    "\n",
    "| GCN        | Global Linear Model |\n",
    "|------------|---------------------|\n",
    "| 184,472.33 | 186,334.11          |\n",
    "\n",
    "While the error for GCN is smaller, it is likely not significant.  I am not showing error standard deviations across multiple runs here, which I may perform in the future.  For now, my curiosity was satisfies :).   Nonetheless, it is cool that GCN performs comparably out-of-the-box without tuning the number of neighbors or its model parameters. \n",
    "\n",
    "My curiosity was satisfied, but here are future steps I may take, and will update the blog accordingly.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "* Explore the impact of $k$, the number of neighbors when building the graph.\n",
    "* Use more thorough evaluation with multiple random splits.\n",
    "* Explore different graph neural network models.\n",
    "* Explore different feature engineering schemes other than appending the vertex degree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
